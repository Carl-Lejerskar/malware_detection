import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import (
    auc,
    roc_curve,
    roc_auc_score,
    recall_score,
    accuracy_score,
    precision_score,
    confusion_matrix,
    f1_score,
    precision_recall_curve,
)

# note: if this moved beyond a POC I'd be using the Google docstring format + type hints!


def get_model_predictions(model, features):
    """
    Get predictions and probabilities from a trained model.
    """
    predictions = model.predict(features)
    try:
        probabilities = model.predict_proba(features)[:, 1]
    except AttributeError:
        if hasattr(model, "decision_function"):
            probabilities = model.decision_function(features)
        else:
            probabilities = None
    return predictions, probabilities


def calculate_metrics(labels, predictions, probabilities=None):
    """
    Given a set of labels (ground truth) and predictions and
    optionally probabilites, this function calculates a set of metrics
    and returns it as a dict.
    """
    metrics = {
        "precision": precision_score(labels, predictions, zero_division=0),
        "recall": recall_score(labels, predictions, zero_division=0),
        "f1_score": f1_score(labels, predictions, zero_division=0),
        "accuracy": accuracy_score(labels, predictions),
    }

    if probabilities is not None:
        metrics["roc_auc"] = roc_auc_score(labels, probabilities)
        metrics["fpr"], metrics["tpr"], _ = roc_curve(labels, probabilities)
        metrics["prec"], metrics["rec"], _ = precision_recall_curve(
            labels, probabilities
        )
    else:
        metrics["roc_auc"] = None
        metrics.update({"fpr": [0, 1], "tpr": [0, 1], "prec": [0, 1], "rec": [0, 1]})
    return metrics


def get_metrics(model, features, labels):
    """
    Given a model, some features, and some ground truth labels, this function
    generates predictions and returns performance metrics.
    """
    predictions, probabilities = get_model_predictions(model, features)
    return calculate_metrics(labels, predictions, probabilities)


def evaluate_model(model, features_train, labels_train, features_test, labels_test):
    """
    This function evaluates a model given training and testing data.
    """
    metrics = {}
    metrics["train"] = get_metrics(model, features_train, labels_train)
    metrics["test"] = get_metrics(model, features_test, labels_test)
    return metrics


def print_metrics(metrics):
    """
    This function prints out metrics given a metric dict created by
    evaluate_model, which contains both train and test metrics for each
    model.
    """
    for name, model_metrics in metrics.items():
        print(f"Model: {name}")
        for dataset in ["train", "test"]:
            data = model_metrics[dataset]
            print(
                f"{dataset.capitalize()} Metrics: Precision: {data['precision']:.4f}, "
                + f"Recall: {data['recall']:.4f}, F1-Score: {data['f1_score']:.4f}, "
                + f"ROC-AUC: {data['roc_auc']:.4f}"
            )
        print("-" * 60)


# Plotting function
def plot_precision_recall_curve(model_metrics):
    """
    This function plots a PR curve given a dictionary of model metrics.
    """
    fig, ax = plt.subplots(figsize=(8, 6))
    for name, metrics in model_metrics.items():
        test_metrics = metrics["test"]
        ax.plot(
            test_metrics["rec"],
            test_metrics["prec"],
            label=f"{name} (AUC = {test_metrics['roc_auc']:.2f})",
        )

    ax.set_xlabel("Recall")
    ax.set_ylabel("Precision")
    ax.set_title("Precision-Recall Curve on Test Data")
    ax.legend(loc="best")
    plt.tight_layout()
    plt.show()


def get_metrics_dfs(metrics):
    """
    This function converts a metrics dictionary into a Pandas DF so it
    is easier to visualize!
    """
    train_rows = []
    test_rows = []

    for name, model_metrics in metrics.items():
        for dataset_type, dataset_metrics in model_metrics.items():
            # Construct row with interpolated dataset_type in the keys
            row = {
                "Model": name,
                f"F1-Score ({dataset_type})": f"{dataset_metrics['f1_score']:.4f}",
                f"Recall ({dataset_type})": f"{dataset_metrics['recall']:.4f}",
                f"Precision ({dataset_type})": f"{dataset_metrics['precision']:.4f}",
                f"Accuracy ({dataset_type})": f"{dataset_metrics['accuracy']:.4f}",
                f"ROC-AUC ({dataset_type})": (
                    f"{dataset_metrics['roc_auc']:.4f}"
                    if dataset_metrics["roc_auc"] is not None
                    else "N/A"
                ),
            }

            if dataset_type == "train":
                train_rows.append(row)
            else:  # Assuming the only other option is 'test'
                test_rows.append(row)

    # Create DataFrames for train and test metrics
    train_metrics_df = pd.DataFrame(train_rows)
    test_metrics_df = pd.DataFrame(test_rows)

    # Since the 'Model' is a common column, we'll set it as index
    train_metrics_df.set_index("Model", inplace=True)
    test_metrics_df.set_index("Model", inplace=True)

    return train_metrics_df, test_metrics_df
